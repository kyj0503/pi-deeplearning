{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ee640b-6434-45f8-ad20-0d22820e64f9",
   "metadata": {},
   "source": [
    "4차원 데이터 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90556efe-ee70-473c-ba0c-70c5be2a8fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(10, 1, 28, 28) #높이 28 너비 28 채널 1 데이터 10\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174416c0-9923-4162-ab63-156ab1df2c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469230a9-31eb-4ac3-a24e-bdde50327d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.95619900e-01, 6.84264127e-01, 8.87198341e-01, 2.60373917e-01,\n",
       "        3.39625392e-01, 6.32548171e-01, 6.53271128e-01, 2.04084097e-01,\n",
       "        7.29174832e-01, 1.60594785e-01, 5.57770820e-01, 8.71882267e-01,\n",
       "        5.27569602e-01, 1.99991744e-01, 7.37171593e-01, 1.78635462e-01,\n",
       "        7.69145098e-01, 4.70487136e-01, 2.43015874e-01, 4.32862960e-01,\n",
       "        8.05153997e-02, 6.68980014e-01, 5.55812428e-01, 7.16438258e-01,\n",
       "        9.27367115e-01, 8.16486130e-02, 1.35170819e-02, 3.29020028e-01],\n",
       "       [2.59080951e-01, 4.32251657e-01, 2.81063579e-02, 8.41660956e-01,\n",
       "        8.97381448e-01, 5.14286477e-01, 4.20626943e-01, 3.21352246e-01,\n",
       "        8.24584873e-01, 5.47405927e-01, 4.82773458e-04, 3.59723505e-01,\n",
       "        9.01920111e-01, 9.95464104e-02, 4.63101196e-01, 8.34734159e-01,\n",
       "        4.80797985e-01, 5.34340869e-01, 6.66952473e-02, 8.04141616e-03,\n",
       "        1.01887489e-01, 4.60069740e-01, 5.60476165e-01, 1.78423621e-01,\n",
       "        8.07273740e-01, 2.11812492e-02, 2.85201528e-02, 2.08197971e-01],\n",
       "       [7.28484262e-01, 6.07012943e-01, 6.34252732e-01, 3.78132548e-02,\n",
       "        6.72633732e-01, 7.18825308e-01, 1.77875832e-01, 7.95367745e-01,\n",
       "        1.74992443e-01, 6.63998706e-01, 6.29053219e-01, 9.00185495e-01,\n",
       "        1.23542673e-01, 2.47724141e-02, 3.60462296e-02, 9.76524350e-01,\n",
       "        4.16589177e-01, 7.49965794e-01, 7.93951035e-01, 6.84122115e-02,\n",
       "        9.57300525e-01, 5.76253331e-01, 8.96260548e-01, 9.91421257e-01,\n",
       "        9.79770177e-01, 1.06871232e-01, 6.03788915e-01, 7.14327969e-01],\n",
       "       [6.60414112e-01, 8.08704271e-01, 7.65075982e-01, 8.82917748e-01,\n",
       "        1.60919535e-01, 5.31726976e-01, 3.18297468e-02, 3.84472778e-01,\n",
       "        4.67105376e-01, 6.23355030e-02, 9.87231445e-01, 9.68992113e-01,\n",
       "        4.74917774e-02, 5.96461081e-01, 9.97315516e-01, 5.47576009e-01,\n",
       "        3.34407140e-01, 5.48408562e-01, 5.83377157e-01, 8.74569210e-01,\n",
       "        4.93460443e-01, 4.36645342e-01, 1.28728791e-01, 2.19072275e-01,\n",
       "        3.83298243e-01, 9.35513713e-01, 1.30233135e-01, 6.52718760e-01],\n",
       "       [4.23512901e-01, 7.36946405e-01, 7.26076282e-01, 3.22153135e-01,\n",
       "        5.10088941e-01, 5.06065544e-01, 4.58301153e-01, 3.04546482e-01,\n",
       "        3.71596751e-01, 6.68902852e-01, 1.98863305e-01, 8.13337925e-01,\n",
       "        5.45390602e-01, 9.59279988e-01, 2.60616781e-01, 2.28894882e-01,\n",
       "        1.94620021e-01, 5.40123488e-01, 1.04562717e-01, 8.05840702e-01,\n",
       "        4.20214236e-01, 5.90823984e-01, 9.22388516e-01, 5.63894792e-01,\n",
       "        8.73832842e-02, 5.03364915e-01, 4.72667906e-01, 6.91159259e-01],\n",
       "       [9.37306098e-01, 5.48923732e-01, 5.43091690e-01, 3.69929749e-01,\n",
       "        6.21687146e-01, 4.26984721e-01, 1.22144273e-01, 8.83094852e-02,\n",
       "        7.30020944e-01, 7.41332289e-01, 2.19147232e-01, 7.59264206e-01,\n",
       "        1.31821309e-01, 4.01874265e-02, 8.27805952e-01, 1.88422982e-01,\n",
       "        3.47136566e-01, 9.72264715e-01, 2.98959929e-01, 2.13544696e-01,\n",
       "        6.24231056e-01, 8.03705372e-01, 8.55771151e-01, 1.63504663e-01,\n",
       "        2.24307402e-01, 9.79062851e-01, 1.14129463e-01, 4.13649710e-01],\n",
       "       [4.93457425e-01, 1.28737656e-01, 8.30337089e-01, 6.99145136e-01,\n",
       "        7.93148501e-01, 8.28656586e-01, 7.04760401e-01, 9.68810245e-02,\n",
       "        9.50131098e-01, 6.01580380e-01, 5.27276899e-01, 4.16338761e-01,\n",
       "        8.46167353e-01, 9.02669875e-01, 6.97963814e-01, 8.84905452e-03,\n",
       "        9.09075855e-01, 2.84801739e-02, 5.51015149e-01, 7.36360760e-01,\n",
       "        4.44588316e-02, 5.84945618e-01, 2.38573317e-01, 7.80181281e-01,\n",
       "        7.37561908e-01, 1.55242090e-01, 4.95749453e-01, 7.17165966e-01],\n",
       "       [5.23315257e-01, 5.80340612e-01, 2.01412637e-01, 3.40909993e-02,\n",
       "        7.77983953e-01, 1.90706206e-01, 3.85883151e-01, 3.59834673e-01,\n",
       "        2.49324430e-01, 3.38439580e-01, 3.92811019e-01, 1.99722650e-01,\n",
       "        1.56101743e-02, 6.55408360e-01, 2.96565397e-02, 2.61925950e-01,\n",
       "        5.75777370e-01, 4.15608479e-01, 1.08990253e-01, 4.69617923e-01,\n",
       "        7.53755052e-01, 1.04204179e-01, 6.08642962e-01, 2.03433644e-01,\n",
       "        1.53235364e-01, 1.13915970e-01, 3.88953911e-02, 9.76224788e-01],\n",
       "       [1.37147347e-01, 5.92518505e-01, 7.98307020e-01, 2.28034344e-01,\n",
       "        9.01515557e-02, 6.21089587e-01, 4.47472525e-03, 4.84245716e-01,\n",
       "        3.83401917e-02, 8.35510325e-03, 9.26948797e-01, 9.30254355e-03,\n",
       "        2.53250023e-01, 3.73627918e-01, 4.75177153e-01, 9.38723557e-01,\n",
       "        9.80234577e-01, 7.54804042e-01, 6.72341531e-02, 2.97262415e-01,\n",
       "        2.80284854e-01, 1.62271886e-01, 3.62469640e-01, 4.67204144e-01,\n",
       "        1.55734909e-01, 7.81718181e-01, 7.99943801e-01, 6.71593199e-01],\n",
       "       [2.19896143e-01, 7.87806243e-01, 9.42812507e-01, 2.77622396e-01,\n",
       "        4.95791408e-01, 7.94834027e-01, 9.37113064e-01, 8.48810992e-01,\n",
       "        4.86051412e-01, 3.09463319e-02, 9.61502749e-01, 5.43802902e-01,\n",
       "        7.02073099e-02, 7.88577240e-01, 8.12025133e-01, 8.71121303e-01,\n",
       "        6.56437041e-01, 2.29630527e-01, 3.71866262e-02, 8.80506385e-02,\n",
       "        1.98322705e-01, 9.59014473e-01, 6.10107792e-01, 1.09557408e-01,\n",
       "        7.70279325e-01, 9.19473890e-01, 8.58579930e-01, 1.28599784e-01],\n",
       "       [3.73234506e-01, 3.21886975e-01, 3.14969521e-01, 3.66863041e-01,\n",
       "        2.82687541e-01, 5.44033873e-01, 8.28790548e-03, 5.07253694e-01,\n",
       "        9.54815879e-02, 8.12938200e-01, 9.23080118e-01, 1.76331972e-01,\n",
       "        8.13106669e-01, 7.80930620e-01, 3.92296401e-01, 1.41987009e-01,\n",
       "        6.16170374e-01, 1.72954021e-02, 5.97983773e-01, 2.35718137e-02,\n",
       "        1.03844891e-01, 4.16688724e-01, 9.96951811e-01, 7.29225362e-01,\n",
       "        3.49877591e-01, 5.32719376e-02, 9.33796039e-01, 4.38905701e-01],\n",
       "       [5.65706095e-01, 9.81243993e-01, 2.68283916e-01, 6.55956708e-01,\n",
       "        9.36943128e-01, 7.07538474e-01, 6.78519506e-01, 4.85639687e-02,\n",
       "        5.60215991e-01, 5.37031072e-01, 2.55882284e-01, 8.45823222e-01,\n",
       "        5.14792084e-01, 5.19069665e-01, 9.48858975e-01, 9.81343762e-02,\n",
       "        3.17943812e-01, 1.77445481e-01, 3.22788826e-01, 5.50214077e-01,\n",
       "        4.60214972e-01, 2.59569025e-02, 1.27963825e-01, 1.71034912e-01,\n",
       "        9.76645544e-01, 6.34794344e-01, 6.70463098e-01, 2.77001110e-01],\n",
       "       [1.89270978e-01, 4.52006527e-01, 7.96200147e-01, 1.16100613e-01,\n",
       "        3.33232739e-01, 3.21191549e-01, 6.84378865e-01, 2.00647820e-01,\n",
       "        8.04971587e-01, 1.41662225e-01, 4.79997845e-01, 2.47568968e-01,\n",
       "        2.75378798e-01, 9.67171393e-01, 8.67240201e-01, 7.69439874e-01,\n",
       "        9.50604273e-01, 5.05650353e-01, 6.28404493e-01, 1.90213859e-01,\n",
       "        7.95290376e-01, 5.99959610e-01, 2.53727947e-01, 9.42890447e-01,\n",
       "        9.99230811e-01, 5.28086308e-01, 1.49197077e-01, 2.80162989e-01],\n",
       "       [9.70506028e-01, 3.50334678e-01, 6.01901723e-01, 1.61613192e-01,\n",
       "        1.42975011e-01, 5.33004341e-01, 5.76101992e-01, 8.95643507e-01,\n",
       "        6.01539552e-01, 5.09171586e-01, 3.78970624e-01, 8.49739848e-01,\n",
       "        6.12451751e-01, 8.97157026e-01, 9.47043163e-01, 6.61114688e-01,\n",
       "        7.92428744e-01, 1.08279721e-01, 3.34785280e-02, 4.60712375e-01,\n",
       "        7.67786162e-01, 4.67619840e-01, 9.90639680e-01, 3.14688867e-02,\n",
       "        1.17266862e-01, 3.75275832e-03, 2.43653556e-01, 7.05062820e-01],\n",
       "       [6.35299271e-01, 4.95632554e-01, 8.94213537e-01, 6.65402540e-01,\n",
       "        3.70850991e-01, 2.54369704e-01, 1.89694786e-01, 9.23455358e-01,\n",
       "        3.24106423e-01, 2.62627381e-01, 8.50014319e-01, 3.50586095e-01,\n",
       "        1.12021830e-01, 5.80441599e-01, 5.57861923e-03, 4.85373466e-01,\n",
       "        5.72519970e-01, 7.57092497e-01, 5.80898307e-01, 3.35508626e-01,\n",
       "        5.23870167e-01, 2.76835188e-01, 4.60808945e-01, 8.96562244e-01,\n",
       "        6.82471869e-01, 3.18392026e-01, 2.11777060e-01, 5.87492913e-01],\n",
       "       [3.80382806e-01, 2.92975439e-01, 1.89667325e-01, 7.38444464e-01,\n",
       "        4.03425321e-01, 4.21146626e-01, 5.27350178e-01, 5.95115128e-02,\n",
       "        6.89937602e-01, 2.79753046e-02, 7.67273305e-01, 3.62228202e-01,\n",
       "        9.69580081e-01, 1.89115414e-01, 4.18949699e-01, 1.17711059e-01,\n",
       "        3.87645858e-01, 9.48911466e-01, 7.86385314e-01, 5.58632283e-02,\n",
       "        5.98597512e-01, 7.04190116e-01, 5.76920239e-01, 5.20416733e-02,\n",
       "        2.69715048e-01, 5.20007143e-01, 5.46672208e-01, 8.12603098e-01],\n",
       "       [3.75360123e-01, 6.67610315e-01, 1.35825119e-01, 4.39366219e-02,\n",
       "        1.64198789e-01, 9.51007387e-01, 8.15801008e-01, 2.96150688e-01,\n",
       "        5.84929051e-01, 6.04823390e-02, 7.00569882e-01, 1.53931232e-02,\n",
       "        2.06187321e-01, 3.78952332e-01, 2.98620255e-01, 6.44239648e-03,\n",
       "        2.91197621e-01, 6.42756105e-01, 1.28368982e-01, 6.72060260e-01,\n",
       "        5.73973957e-01, 5.10472447e-01, 5.27953943e-01, 4.80369979e-01,\n",
       "        6.26279262e-01, 6.47961767e-01, 8.76785410e-01, 8.70734845e-01],\n",
       "       [9.36571070e-01, 4.85637288e-01, 3.73581275e-01, 9.83860317e-01,\n",
       "        5.62291174e-01, 1.87517153e-01, 5.80147852e-01, 7.32600475e-01,\n",
       "        8.12446757e-01, 4.67190451e-01, 8.27797074e-01, 4.09382516e-01,\n",
       "        2.29385676e-01, 9.64515595e-01, 5.67735863e-01, 1.82423078e-01,\n",
       "        8.30087568e-01, 8.19746093e-01, 4.53487685e-01, 7.66065089e-01,\n",
       "        7.14916430e-01, 1.99025225e-01, 2.75423885e-01, 3.42242021e-01,\n",
       "        4.31948037e-01, 4.50243062e-01, 5.88908872e-01, 6.49423610e-01],\n",
       "       [7.80267034e-01, 6.59505033e-01, 1.05840862e-01, 2.12337277e-01,\n",
       "        3.71084843e-01, 1.79167054e-01, 5.88946903e-01, 4.98388388e-01,\n",
       "        7.72806116e-01, 8.00110724e-01, 6.41067307e-01, 1.21988012e-01,\n",
       "        5.70350352e-01, 2.47646287e-01, 1.69436750e-01, 1.16982016e-01,\n",
       "        6.69806096e-01, 7.00079930e-01, 1.87906993e-02, 6.94353746e-01,\n",
       "        7.46182834e-01, 3.42213557e-01, 4.12485930e-01, 8.79328496e-01,\n",
       "        5.33144756e-01, 9.17353755e-02, 5.09544601e-02, 4.00304045e-01],\n",
       "       [2.31895724e-01, 4.50629649e-01, 1.62894700e-01, 1.51379267e-01,\n",
       "        5.76503742e-01, 5.14320575e-01, 6.02170847e-01, 3.23230520e-01,\n",
       "        2.60392089e-01, 2.65387819e-02, 5.78710090e-01, 3.41015996e-01,\n",
       "        2.26972031e-01, 4.88723457e-01, 2.01793766e-01, 6.24262053e-01,\n",
       "        7.72593968e-01, 6.84996598e-01, 9.56808940e-01, 2.79593985e-01,\n",
       "        9.00399942e-01, 6.18576251e-01, 5.79031575e-01, 9.01866342e-01,\n",
       "        1.52518506e-01, 8.31640632e-01, 3.68304384e-01, 7.32936833e-01],\n",
       "       [3.41651833e-01, 4.40832928e-01, 8.23900267e-01, 7.60430549e-01,\n",
       "        1.04677278e-01, 1.30498241e-01, 2.37077019e-02, 4.49973799e-01,\n",
       "        8.36261165e-01, 7.47136835e-01, 5.21810720e-01, 3.66722575e-01,\n",
       "        4.03453488e-01, 5.08452331e-01, 9.24305166e-01, 4.92688225e-01,\n",
       "        5.16380032e-01, 5.06451506e-01, 3.02303000e-01, 5.69399834e-01,\n",
       "        9.84995152e-01, 5.34478258e-01, 9.07157560e-01, 6.12366928e-01,\n",
       "        1.06793038e-02, 2.20286388e-01, 3.90135708e-01, 3.93516721e-01],\n",
       "       [2.90119958e-01, 2.97815313e-01, 3.76547737e-01, 2.17716980e-01,\n",
       "        9.60404506e-01, 6.99810801e-01, 3.49045287e-01, 4.25168534e-01,\n",
       "        4.79516460e-01, 2.32581270e-01, 5.61137070e-01, 6.23042507e-01,\n",
       "        2.74667004e-01, 5.40891977e-01, 5.87532371e-01, 9.43553095e-01,\n",
       "        5.06271948e-01, 9.83606003e-01, 9.01415210e-01, 6.72458132e-01,\n",
       "        8.17865986e-01, 4.22052502e-01, 3.47599447e-01, 3.39411368e-01,\n",
       "        8.40038785e-01, 1.81889551e-01, 2.04473705e-01, 4.65966536e-01],\n",
       "       [1.70607549e-01, 8.50667498e-01, 7.74508417e-01, 7.21524448e-01,\n",
       "        1.09336590e-01, 5.08952490e-01, 4.84058230e-01, 7.86654945e-01,\n",
       "        6.95400860e-01, 5.30142619e-01, 8.72169190e-01, 3.99208583e-01,\n",
       "        7.67092767e-02, 9.29399992e-01, 8.74470546e-01, 9.28002954e-01,\n",
       "        9.89955912e-02, 2.15919305e-03, 4.25721544e-02, 8.14995050e-02,\n",
       "        3.51999145e-01, 7.77883486e-02, 9.92711693e-01, 1.47381273e-01,\n",
       "        6.49657877e-01, 3.98632751e-01, 5.31906202e-01, 6.15570522e-01],\n",
       "       [9.80614808e-01, 5.19791498e-01, 7.32664668e-01, 9.12597922e-01,\n",
       "        3.21172113e-01, 7.87983873e-01, 9.56983212e-01, 7.45854637e-01,\n",
       "        1.98047929e-01, 6.96389029e-01, 9.44962104e-01, 3.73398254e-01,\n",
       "        9.18773680e-01, 6.61275614e-01, 8.23955903e-01, 2.33827958e-01,\n",
       "        1.33428728e-01, 2.54332291e-01, 3.61403969e-01, 7.02716179e-01,\n",
       "        5.71013675e-01, 4.99679797e-01, 9.22537544e-01, 2.51723883e-01,\n",
       "        9.97984998e-01, 5.76059523e-01, 4.09604594e-01, 4.47796021e-01],\n",
       "       [1.06343701e-01, 3.49976176e-01, 9.06141077e-01, 5.24290635e-01,\n",
       "        2.00625293e-01, 4.14977511e-03, 6.04707099e-01, 5.69896230e-01,\n",
       "        4.92790769e-01, 8.17971311e-01, 9.11413759e-01, 2.88019259e-01,\n",
       "        5.07230266e-01, 6.75054156e-01, 2.94526894e-01, 6.60378605e-01,\n",
       "        5.15042294e-01, 1.73398667e-01, 5.20305051e-01, 2.45166377e-01,\n",
       "        7.97355244e-01, 2.94610250e-01, 2.91936436e-01, 7.01130397e-01,\n",
       "        3.83805391e-03, 2.92805481e-01, 9.36849889e-01, 5.97529005e-01],\n",
       "       [9.16264728e-01, 6.86978478e-01, 2.10201128e-01, 7.65659439e-01,\n",
       "        7.90298147e-01, 3.50337699e-01, 8.60569665e-01, 3.43045026e-01,\n",
       "        3.69085187e-01, 6.13579309e-01, 6.14738895e-01, 2.43746275e-01,\n",
       "        9.10178673e-01, 2.81724485e-01, 3.23423654e-01, 1.90800556e-01,\n",
       "        3.20348475e-01, 7.19069145e-01, 8.89447849e-01, 7.33816118e-01,\n",
       "        5.84275419e-01, 1.30611588e-01, 7.95861467e-02, 1.36726469e-01,\n",
       "        8.11070147e-01, 6.82901379e-01, 2.25800628e-01, 1.35244934e-01],\n",
       "       [8.36822356e-01, 1.95717529e-01, 6.13830369e-01, 3.09346290e-01,\n",
       "        6.72049672e-01, 5.12183654e-01, 2.99115078e-01, 7.87823055e-01,\n",
       "        9.00637030e-01, 7.96927695e-01, 9.57644939e-01, 6.05111492e-01,\n",
       "        7.55877000e-01, 3.42030624e-01, 7.84428966e-01, 3.75667382e-01,\n",
       "        7.40529366e-01, 9.35370637e-01, 1.23354339e-01, 8.71403284e-01,\n",
       "        9.31040912e-01, 7.27538577e-01, 9.70444663e-01, 9.71437505e-01,\n",
       "        3.56074520e-01, 6.46102273e-01, 1.64137732e-01, 7.25687866e-01],\n",
       "       [7.66527158e-01, 2.39903947e-02, 1.29296526e-01, 3.90261991e-01,\n",
       "        6.09083723e-01, 2.43761053e-01, 2.59062619e-01, 5.96659906e-01,\n",
       "        7.67216345e-01, 9.88877275e-01, 1.79860153e-01, 4.38151476e-01,\n",
       "        1.50758814e-02, 2.48971751e-01, 9.45952135e-01, 7.29606684e-01,\n",
       "        7.16562585e-01, 5.05797977e-01, 2.90191923e-01, 7.26890944e-02,\n",
       "        9.23638055e-01, 4.94728606e-01, 5.30765572e-01, 1.91438607e-01,\n",
       "        9.52431983e-01, 7.63577093e-01, 9.01235146e-01, 2.43935950e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c530363-e7ee-47a5-946a-61a1af2bac8f",
   "metadata": {},
   "source": [
    "합성곱 연산 구현의 문제점 - for문을 겹겹이 써야함\n",
    "넘파이에 for문을 사용하면 성능이 떨어진다는 단점도 있음\n",
    "\n",
    "im2col 함수를 사용한 구현\n",
    "입력 데이터를 필터링하기 좋게 전해하는 함수\n",
    "3차원 입력 데이터에 im2col을 적용하면 2차원 행렬도 바뀜\n",
    "메모리를 더 많이 소비하는 단점이 있음\n",
    "문제를 행렬 계산으로 만들면 선형 대수 라이브러리를 활용해 효율을 높일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003b416-c575-421c-a845-34cf21d86c75",
   "metadata": {},
   "source": [
    "im2col 함수의 인터페이스\n",
    "im2col(input_data, filter_h, filter_w, stride=1, pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12301e57-5d09-43e4-b567-c11dcff9ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b09bbb9-0ef5-4696-8651-85b9a74a88c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7) # 데이터 수, 채널 수, 높이, 너비\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be7d06d2-2855-4e35-b212-4d302420a11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.rand(10, 3, 7, 7) # 데이터 수, 채널 수, 높이, 너비\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e21a3-8952-49dd-8067-343867b4eb7e",
   "metadata": {},
   "source": [
    "합성곱 계층은 필터(가중치), 편향, 스트라이드, 패딩을 인수로 받아 초기화한다.\n",
    "필터는 (FN, C, FH, FW)의 4차원 형상이다. FN = 필터의 개수, C = 채널, FH = 필터의 높이, FW = 필터의 너비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2e458d-a6cd-40ca-a7c5-7da42f866839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad) # 1. 입력데이터를 im2col로 전개\n",
    "        col_W =self.W.reshape(FN, -1).T # 2. 필터도 reshape을 사용해 2차원 배열로 전개\n",
    "                                        # reshape에 -1을 지정하면 다차원 배열의 원소 수가 변환 후에도 똑같이 유지 되도록 적절히 묶어줌\n",
    "        out = np.dot(col, col_W) + self.b # 3. 전개한 두 행렬의 곱을 구함\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) # 4. 출력 데이터를 적절한 형상으로 바꿔줌\n",
    "                                                                    # transpose 함수를 사용하여 다차원 배열의 축 순서를 바꿈\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2d786-b70a-48c9-a80d-54bd17f144a8",
   "metadata": {},
   "source": [
    "폴링 계층의 구현 세 단계\n",
    "1. 입력 데이터를 전개한다.\n",
    "2. 행별 최댓값을 구한다.\n",
    "3. 적절한 모양으로 성형한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c6e78ce-a034-4b80-abd6-33472b76886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool.h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool.w) / self.stride)\n",
    "\n",
    "        # (1) 전개\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        # (2) 최댓값\n",
    "        out = np.max(col, axis=1)\n",
    "\n",
    "        # (3) 성형\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abefaa0-7584-4c46-b833-7e35f88edcee",
   "metadata": {},
   "source": [
    "CNN 계층 구현하기\n",
    "다음과 같은 CNN을 구현한다.\n",
    "Convolution(입력 데이터의 특징을 추출) -> ReLU -> Pooling(적절한 모양으로 성형) -> Affine(행렬곱 연산) -> ReLU -> Affine -> Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a3f6178-7057-4e05-815d-87dbb54c03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25ea6cfd-ac71-4ea6-b9ca-7deee7e84150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2998736906960646\n",
      "=== epoch:1, train acc:0.245, test acc:0.167 ===\n",
      "train loss:2.2969574745443717\n",
      "train loss:2.292817184859777\n",
      "train loss:2.285985967193147\n",
      "train loss:2.277800926408448\n",
      "train loss:2.2586606879580704\n",
      "train loss:2.2456149068433797\n",
      "train loss:2.22573383630323\n",
      "train loss:2.2237721108425945\n",
      "train loss:2.1887619911291476\n",
      "train loss:2.1607581239353046\n",
      "train loss:2.125691619595951\n",
      "train loss:2.048228123977685\n",
      "train loss:1.9819903605538038\n",
      "train loss:1.936836969611781\n",
      "train loss:1.842447593116229\n",
      "train loss:1.8113295100776778\n",
      "train loss:1.6880248103143083\n",
      "train loss:1.636505457809488\n",
      "train loss:1.5544841339045161\n",
      "train loss:1.468885208613945\n",
      "train loss:1.3129450550650807\n",
      "train loss:1.321690012605316\n",
      "train loss:1.341452893286671\n",
      "train loss:1.0537773765825644\n",
      "train loss:1.050775823018068\n",
      "train loss:0.9966729824070447\n",
      "train loss:0.8260071139699728\n",
      "train loss:0.8519184723667043\n",
      "train loss:0.9679702475749475\n",
      "train loss:0.8158377563714373\n",
      "train loss:0.7817146870004609\n",
      "train loss:0.8361058854040021\n",
      "train loss:0.7303217667171487\n",
      "train loss:0.6571819918707585\n",
      "train loss:0.7602933374794\n",
      "train loss:0.8283434923408491\n",
      "train loss:0.7009618525283524\n",
      "train loss:0.673757033037845\n",
      "train loss:0.6118445753882242\n",
      "train loss:0.7012096502669114\n",
      "train loss:0.4982177888368724\n",
      "train loss:0.631512560572353\n",
      "train loss:0.6361814941516235\n",
      "train loss:0.5755578572532966\n",
      "train loss:0.4547504757063682\n",
      "train loss:0.729693401940539\n",
      "train loss:0.5436743220267727\n",
      "train loss:0.7574103174157597\n",
      "train loss:0.4203968768257911\n",
      "train loss:0.3006069511540988\n",
      "=== epoch:2, train acc:0.824, test acc:0.79 ===\n",
      "train loss:0.4055913138062898\n",
      "train loss:0.6961474604889006\n",
      "train loss:0.43377963661617003\n",
      "train loss:0.45525263495944146\n",
      "train loss:0.42709532868706956\n",
      "train loss:0.3560073239081869\n",
      "train loss:0.5672851092239448\n",
      "train loss:0.501688665364297\n",
      "train loss:0.39791698491936883\n",
      "train loss:0.3704276264694954\n",
      "train loss:0.43922634478894307\n",
      "train loss:0.661257564762892\n",
      "train loss:0.43598799581180925\n",
      "train loss:0.46450629286850337\n",
      "train loss:0.5362006350034689\n",
      "train loss:0.3034550586668251\n",
      "train loss:0.28747975334182435\n",
      "train loss:0.5129558459782776\n",
      "train loss:0.33296831598722987\n",
      "train loss:0.5240554749001811\n",
      "train loss:0.4999980702349554\n",
      "train loss:0.3935957595329115\n",
      "train loss:0.5503612327282377\n",
      "train loss:0.4314150096801454\n",
      "train loss:0.3219293295389714\n",
      "train loss:0.3109680459073791\n",
      "train loss:0.3825574205804495\n",
      "train loss:0.4417452758808704\n",
      "train loss:0.321685099615108\n",
      "train loss:0.3751653616081391\n",
      "train loss:0.2917589630435566\n",
      "train loss:0.37379571915374243\n",
      "train loss:0.2971387710808879\n",
      "train loss:0.3794600238924598\n",
      "train loss:0.5796020057416756\n",
      "train loss:0.47826285022894893\n",
      "train loss:0.4786928253974465\n",
      "train loss:0.3820631971127182\n",
      "train loss:0.32226510092038857\n",
      "train loss:0.4187653598432401\n",
      "train loss:0.319178395005745\n",
      "train loss:0.2080158492784103\n",
      "train loss:0.3957130582354559\n",
      "train loss:0.28638968203298937\n",
      "train loss:0.27848476745562056\n",
      "train loss:0.26962886424443205\n",
      "train loss:0.3028408609137722\n",
      "train loss:0.2868328958947021\n",
      "train loss:0.26150303297141453\n",
      "train loss:0.34759290923420993\n",
      "=== epoch:3, train acc:0.885, test acc:0.871 ===\n",
      "train loss:0.19124708060689755\n",
      "train loss:0.5500859860317509\n",
      "train loss:0.392477825204956\n",
      "train loss:0.25172060401950846\n",
      "train loss:0.20894184989059436\n",
      "train loss:0.39750789388220353\n",
      "train loss:0.3167216542324438\n",
      "train loss:0.44564780545099475\n",
      "train loss:0.23988673074965436\n",
      "train loss:0.3935926948674271\n",
      "train loss:0.23281461469542003\n",
      "train loss:0.23757389256606398\n",
      "train loss:0.2031472712870469\n",
      "train loss:0.163858921316058\n",
      "train loss:0.32511393163348773\n",
      "train loss:0.42192629746203886\n",
      "train loss:0.32759725052028865\n",
      "train loss:0.3136439251603172\n",
      "train loss:0.3170665302759494\n",
      "train loss:0.33481953872467185\n",
      "train loss:0.19955919804520103\n",
      "train loss:0.296599962756328\n",
      "train loss:0.2803884820746372\n",
      "train loss:0.5025664629553414\n",
      "train loss:0.2697404014019159\n",
      "train loss:0.2897965805229675\n",
      "train loss:0.45299276310122033\n",
      "train loss:0.3090353155017269\n",
      "train loss:0.29003079613928784\n",
      "train loss:0.36913295983800576\n",
      "train loss:0.424208517874413\n",
      "train loss:0.41189065767993055\n",
      "train loss:0.25388005517271534\n",
      "train loss:0.27475413659332476\n",
      "train loss:0.225307057808127\n",
      "train loss:0.23112565570270197\n",
      "train loss:0.1689927474433536\n",
      "train loss:0.2686535903850493\n",
      "train loss:0.20141887191698138\n",
      "train loss:0.43075434584179595\n",
      "train loss:0.19148253029968226\n",
      "train loss:0.34057947641842184\n",
      "train loss:0.2976595648318306\n",
      "train loss:0.20301911225812233\n",
      "train loss:0.24920952824866094\n",
      "train loss:0.18474589650255396\n",
      "train loss:0.36232729704136274\n",
      "train loss:0.2610244975202562\n",
      "train loss:0.34781260158903415\n",
      "train loss:0.24778224158327739\n",
      "=== epoch:4, train acc:0.902, test acc:0.89 ===\n",
      "train loss:0.30539982075348865\n",
      "train loss:0.35318443473264877\n",
      "train loss:0.22830522725542038\n",
      "train loss:0.2719136070564002\n",
      "train loss:0.34791090380414985\n",
      "train loss:0.20496513926164062\n",
      "train loss:0.45875748919909987\n",
      "train loss:0.2467901621625021\n",
      "train loss:0.3324181918734957\n",
      "train loss:0.2120463982204142\n",
      "train loss:0.20967653642004605\n",
      "train loss:0.19231366588129212\n",
      "train loss:0.40306162124863654\n",
      "train loss:0.37754874639479163\n",
      "train loss:0.3183046473851361\n",
      "train loss:0.3086548297185015\n",
      "train loss:0.2666258195708421\n",
      "train loss:0.39036401426136846\n",
      "train loss:0.522000957031944\n",
      "train loss:0.2831326687930842\n",
      "train loss:0.32161350142937806\n",
      "train loss:0.21322499253991187\n",
      "train loss:0.3108203304929194\n",
      "train loss:0.270561017607825\n",
      "train loss:0.29844334014139756\n",
      "train loss:0.3259817027176071\n",
      "train loss:0.1192555268592051\n",
      "train loss:0.33898933310162993\n",
      "train loss:0.21359060209740638\n",
      "train loss:0.1908016854051184\n",
      "train loss:0.21484732707255053\n",
      "train loss:0.23187821553031887\n",
      "train loss:0.20550880753924944\n",
      "train loss:0.26902035250713224\n",
      "train loss:0.23712441900746487\n",
      "train loss:0.24653439851722006\n",
      "train loss:0.3607141126107708\n",
      "train loss:0.28727229702952195\n",
      "train loss:0.20785215176764535\n",
      "train loss:0.36079998156436766\n",
      "train loss:0.27631134693753034\n",
      "train loss:0.23022947857945852\n",
      "train loss:0.24305161404931763\n",
      "train loss:0.18604815748215317\n",
      "train loss:0.28255743200454986\n",
      "train loss:0.20591136918907238\n",
      "train loss:0.2403820930404731\n",
      "train loss:0.19064767363023172\n",
      "train loss:0.25333118160380763\n",
      "train loss:0.32023191824976655\n",
      "=== epoch:5, train acc:0.91, test acc:0.904 ===\n",
      "train loss:0.21751992178049648\n",
      "train loss:0.22691929112638148\n",
      "train loss:0.2157324670857936\n",
      "train loss:0.26364843257204623\n",
      "train loss:0.1743410645754265\n",
      "train loss:0.1858292566895514\n",
      "train loss:0.21253638286241888\n",
      "train loss:0.24255164202461146\n",
      "train loss:0.40079373023164294\n",
      "train loss:0.14481495163122496\n",
      "train loss:0.09916218737347389\n",
      "train loss:0.1896408454412557\n",
      "train loss:0.22428525245941167\n",
      "train loss:0.1799134200497991\n",
      "train loss:0.20704598770643382\n",
      "train loss:0.29717304154084767\n",
      "train loss:0.2579602207961021\n",
      "train loss:0.17969582460756128\n",
      "train loss:0.10901091781088418\n",
      "train loss:0.10783886750216035\n",
      "train loss:0.20070438683057112\n",
      "train loss:0.24711764129872638\n",
      "train loss:0.21248449284246299\n",
      "train loss:0.0950929644429975\n",
      "train loss:0.313873302335024\n",
      "train loss:0.12251222997239145\n",
      "train loss:0.21524461305288795\n",
      "train loss:0.23572197678635834\n",
      "train loss:0.20034727783691458\n",
      "train loss:0.23436143290451739\n",
      "train loss:0.18033094621996798\n",
      "train loss:0.3384936228016824\n",
      "train loss:0.2245465320557247\n",
      "train loss:0.08241864064611279\n",
      "train loss:0.4440810025562347\n",
      "train loss:0.2054256060205965\n",
      "train loss:0.36455126602761867\n",
      "train loss:0.196628096178242\n",
      "train loss:0.15968231339393577\n",
      "train loss:0.18384560422935992\n",
      "train loss:0.12794215641652099\n",
      "train loss:0.11644415416622474\n",
      "train loss:0.15095702640944442\n",
      "train loss:0.1600725606782599\n",
      "train loss:0.28638660586716713\n",
      "train loss:0.16765777630396314\n",
      "train loss:0.24073864471423845\n",
      "train loss:0.24123489978217083\n",
      "train loss:0.1505191302201324\n",
      "train loss:0.2561263490994853\n",
      "=== epoch:6, train acc:0.927, test acc:0.917 ===\n",
      "train loss:0.17700316692458667\n",
      "train loss:0.225374032873437\n",
      "train loss:0.14153266086850128\n",
      "train loss:0.180600351514712\n",
      "train loss:0.13800052806537766\n",
      "train loss:0.2934795096626746\n",
      "train loss:0.1552807944554292\n",
      "train loss:0.08716186848064034\n",
      "train loss:0.11746625840939601\n",
      "train loss:0.15989696684159957\n",
      "train loss:0.16150011120882346\n",
      "train loss:0.22369417650078435\n",
      "train loss:0.24996648162738236\n",
      "train loss:0.22077981463510793\n",
      "train loss:0.1200344899008135\n",
      "train loss:0.20407498307192423\n",
      "train loss:0.15543504475686343\n",
      "train loss:0.10318695606120733\n",
      "train loss:0.2734412625864204\n",
      "train loss:0.260477006821659\n",
      "train loss:0.2367441912507959\n",
      "train loss:0.25846106964013105\n",
      "train loss:0.19145057278125535\n",
      "train loss:0.2318121318330003\n",
      "train loss:0.19846971668227767\n",
      "train loss:0.31078359515800275\n",
      "train loss:0.17611385160279017\n",
      "train loss:0.11104291233074907\n",
      "train loss:0.1086562858258271\n",
      "train loss:0.1303928932968991\n",
      "train loss:0.09775708336237553\n",
      "train loss:0.156542700486469\n",
      "train loss:0.1615304813672239\n",
      "train loss:0.07091242085055076\n",
      "train loss:0.10939512832445723\n",
      "train loss:0.12767240328789706\n",
      "train loss:0.1154526561059647\n",
      "train loss:0.10703180403638438\n",
      "train loss:0.16331951985409499\n",
      "train loss:0.3792644505540909\n",
      "train loss:0.18403567015718977\n",
      "train loss:0.11228915889245018\n",
      "train loss:0.25753750880442544\n",
      "train loss:0.16616999802336518\n",
      "train loss:0.25113834870668156\n",
      "train loss:0.2090761534964149\n",
      "train loss:0.19505853929835293\n",
      "train loss:0.09611251187104354\n",
      "train loss:0.09697878471398536\n",
      "train loss:0.11814870333742912\n",
      "=== epoch:7, train acc:0.94, test acc:0.926 ===\n",
      "train loss:0.2555995569119236\n",
      "train loss:0.14598806391999022\n",
      "train loss:0.1590255995913708\n",
      "train loss:0.1698800306919149\n",
      "train loss:0.23043285573998556\n",
      "train loss:0.20296795637596116\n",
      "train loss:0.09602170720835827\n",
      "train loss:0.11260645270213741\n",
      "train loss:0.11522487892515876\n",
      "train loss:0.11430882602759952\n",
      "train loss:0.1626317294034305\n",
      "train loss:0.23030644992466942\n",
      "train loss:0.17985796400485402\n",
      "train loss:0.12766219976889154\n",
      "train loss:0.18545638645621396\n",
      "train loss:0.14382627191304215\n",
      "train loss:0.12785239695068865\n",
      "train loss:0.1153709660941468\n",
      "train loss:0.16125927822897793\n",
      "train loss:0.16421281830093284\n",
      "train loss:0.1317652859260008\n",
      "train loss:0.06441919593958004\n",
      "train loss:0.1758861167116208\n",
      "train loss:0.0881374726015003\n",
      "train loss:0.07687543123790604\n",
      "train loss:0.12655267361845696\n",
      "train loss:0.11790473533262427\n",
      "train loss:0.2293443961245762\n",
      "train loss:0.1525408482785219\n",
      "train loss:0.17555159451588587\n",
      "train loss:0.1298593345499998\n",
      "train loss:0.16425826082220227\n",
      "train loss:0.13090550481686686\n",
      "train loss:0.21093249346203372\n",
      "train loss:0.1401004268618542\n",
      "train loss:0.11692985525575676\n",
      "train loss:0.17555426837915772\n",
      "train loss:0.1356293490199295\n",
      "train loss:0.09212441461114061\n",
      "train loss:0.1350076182718286\n",
      "train loss:0.16231500811323357\n",
      "train loss:0.12278879643034621\n",
      "train loss:0.14139435718028506\n",
      "train loss:0.1267478284177776\n",
      "train loss:0.184531742365172\n",
      "train loss:0.18084354678071604\n",
      "train loss:0.17627294836536717\n",
      "train loss:0.11922025491950883\n",
      "train loss:0.10432270437285414\n",
      "train loss:0.2295290145585714\n",
      "=== epoch:8, train acc:0.941, test acc:0.922 ===\n",
      "train loss:0.19543554846825245\n",
      "train loss:0.14416083973336508\n",
      "train loss:0.2327342180815022\n",
      "train loss:0.11939138384897509\n",
      "train loss:0.10921637458782182\n",
      "train loss:0.11271451328018571\n",
      "train loss:0.17303829293657724\n",
      "train loss:0.10252004475968966\n",
      "train loss:0.18503564041849496\n",
      "train loss:0.33219999347045104\n",
      "train loss:0.2177441798247143\n",
      "train loss:0.1429259382959589\n",
      "train loss:0.14938151863402452\n",
      "train loss:0.08793532719567898\n",
      "train loss:0.14881724187139106\n",
      "train loss:0.12316877267448101\n",
      "train loss:0.14318103963922268\n",
      "train loss:0.17032985491336528\n",
      "train loss:0.19015306219318692\n",
      "train loss:0.17587938184771235\n",
      "train loss:0.22917382051724386\n",
      "train loss:0.18909504363374105\n",
      "train loss:0.19919732376459465\n",
      "train loss:0.23601939883899425\n",
      "train loss:0.08572285622053936\n",
      "train loss:0.09592491497104617\n",
      "train loss:0.1363620621533112\n",
      "train loss:0.22662650532646514\n",
      "train loss:0.16195949443531582\n",
      "train loss:0.13003664444160312\n",
      "train loss:0.11102597523885924\n",
      "train loss:0.12373605967747761\n",
      "train loss:0.1664947805324513\n",
      "train loss:0.13540704652574032\n",
      "train loss:0.14364236950836248\n",
      "train loss:0.12383614030518825\n",
      "train loss:0.11437607242119557\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 26\u001b[0m\n\u001b[0;32m     18\u001b[0m network \u001b[38;5;241m=\u001b[39m SimpleConvNet(input_dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m), \n\u001b[0;32m     19\u001b[0m                         conv_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_num\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m},\n\u001b[0;32m     20\u001b[0m                         hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, weight_init_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[0;32m     23\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39mmax_epochs, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     24\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m},\n\u001b[0;32m     25\u001b[0m                   evaluate_sample_num_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 매개변수 보존\u001b[39;00m\n\u001b[0;32m     29\u001b[0m network\u001b[38;5;241m.\u001b[39msave_params(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\source\\pi-deeplearning\\common\\trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m---> 71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[0;32m     73\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32m~\\source\\pi-deeplearning\\common\\trainer.py:47\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[1;32m---> 47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss_list\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(loss))\n",
      "Cell \u001b[1;32mIn[15], line 75\u001b[0m, in \u001b[0;36mSimpleConvNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"손실 함수를 구한다.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    t : 정답 레이블\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
      "Cell \u001b[1;32mIn[15], line 63\u001b[0m, in \u001b[0;36mSimpleConvNet.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 63\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\source\\pi-deeplearning\\common\\layers.py:261\u001b[0m, in \u001b[0;36mPooling.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    258\u001b[0m out_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (H \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_h) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n\u001b[0;32m    259\u001b[0m out_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (W \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_w) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n\u001b[1;32m--> 261\u001b[0m col \u001b[38;5;241m=\u001b[39m im2col(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad)\n\u001b[0;32m    262\u001b[0m col \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_h\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_w)\n\u001b[0;32m    264\u001b[0m arg_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\source\\pi-deeplearning\\common\\util.py:58\u001b[0m, in \u001b[0;36mim2col\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     55\u001b[0m out_h \u001b[38;5;241m=\u001b[39m (H \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mpad \u001b[38;5;241m-\u001b[39m filter_h)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstride \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     56\u001b[0m out_w \u001b[38;5;241m=\u001b[39m (W \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mpad \u001b[38;5;241m-\u001b[39m filter_w)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstride \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 58\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(input_data, [(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m), (pad, pad), (pad, pad)], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     59\u001b[0m col \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((N, C, filter_h, filter_w, out_h, out_w))\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(filter_h):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\arraypad.py:801\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    798\u001b[0m padded, original_area_slice \u001b[38;5;241m=\u001b[39m _pad_simple(array, pad_width)\n\u001b[0;32m    799\u001b[0m \u001b[38;5;66;03m# And prepare iteration over all dimensions\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# (zipping may be more readable than using enumerate)\u001b[39;00m\n\u001b[1;32m--> 801\u001b[0m axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(padded\u001b[38;5;241m.\u001b[39mndim)\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    804\u001b[0m     values \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df8a7e-302c-4170-beb9-1958e219122d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
